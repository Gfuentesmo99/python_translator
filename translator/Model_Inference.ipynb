{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Español- Numero de palabras:  4050 \t Frase mas larga:  102\n",
      "Ingles - Numero de palabras:  4841 \t Frase mas larga:  105\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%run utils.ipynb\n",
    "#Leemos el dataset reducido\n",
    "es_train, es_train, es_test, en_test,es_texts,en_texts = init_data()\n",
    "\n",
    "\n",
    "#Ahora vamos a tokenizar los diferentes datasets para ello usaremos Keras y su Tokenizer\n",
    "es_tok,en_tok,es_len,en_len,es_vocab,en_vocab = init_tokens(en_texts,es_texts)\n",
    "\n",
    "print(\"Español- Numero de palabras: \", es_vocab, \"\\t Frase mas larga: \", es_len)\n",
    "print(\"Ingles - Numero de palabras: \", en_vocab, \"\\t Frase mas larga: \", en_len)\n",
    "\n",
    "#Dividimos el dataset en entrenamiento y prueba\n",
    "es_train, es_test, en_train, en_test = split_dataset(es_texts,en_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           [(None, 102, 4050)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           [(None, 104, 4841)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_17 (GRU)                    [(None, 128), (None, 1605120     input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_18 (GRU)                    (None, 104, 128)     1908864     input_25[0][0]                   \n",
      "                                                                 gru_17[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 104, 4841)    624489      gru_18[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,138,473\n",
      "Trainable params: 4,138,473\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%run utils.ipynb\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = create_model_inference_train(es_len,en_len,es_vocab,en_vocab)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 72s 8s/step - loss: 8.4663 - acc: 0.4247 - val_loss: 8.3848 - val_acc: 0.9837\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 65s 9s/step - loss: 8.3378 - acc: 0.9640 - val_loss: 8.0294 - val_acc: 0.8004\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 65s 9s/step - loss: 7.5181 - acc: 0.7983 - val_loss: 4.5889 - val_acc: 0.7825\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 55s 8s/step - loss: 3.8621 - acc: 0.7845 - val_loss: 1.9586 - val_acc: 0.7731\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 48s 7s/step - loss: 1.6167 - acc: 0.7754 - val_loss: 0.8729 - val_acc: 0.7731\n"
     ]
    }
   ],
   "source": [
    "%run utils.ipynb\n",
    "\n",
    "#Procesaremos las frases para nuestro modelo que tendra 2 \"inputs\" y un \"output\".\n",
    "#Las entradas son: El one hot vector de las palabras en español y el onehot vector en ingles menos la ultima palabra\n",
    "#La salida es el one hot vector en ingles menos la primera palabra\n",
    "\n",
    "train_inference_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 119, 4937)]       0         \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 [(None, 128), (None, 128) 1945728   \n",
      "=================================================================\n",
      "Total params: 1,945,728\n",
      "Trainable params: 1,945,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           [(None, 1, 4161)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_16 (GRU)                    [(None, 128), (None, 1647744     input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4161)         536769      gru_16[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,184,513\n",
      "Trainable params: 2,184,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-83a4989f3004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0men_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobability_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_tok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0men_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eos\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-83a4989f3004>\u001b[0m in \u001b[0;36mprobability_to_word\u001b[0;34m(tokenizer, prob)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprobability_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Ahora lo siguiente que tenemos que hacer es cargar el modelo entrenado (\"Modelo anterior a este\") y copiar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import numpy as np\n",
    "\n",
    "def word_to_onehot(tokenizer, word, vocab_size):\n",
    "    seq = tokenizer.texts_to_sequences([[word]])\n",
    "    seq = to_categorical(seq, num_classes=vocab_size)\n",
    "    seq = np.expand_dims(seq, axis=1)\n",
    "    return seq\n",
    "\n",
    "def probability_to_word(tokenizer, prob):\n",
    "    word_index = np.argmax(prob[0,:], axis=-1)\n",
    "    word = tokenizer.index_word[word_index]\n",
    "    return word\n",
    "#Ahora lo siguiente que tenemos que hacer es cargar el modelo entrenado (\"Modelo anterior a este\") y copiar\n",
    "#los pesos\n",
    "#En nuestro nuevo modelo hay 3 capas que contienen pesos, la Gru layer del encoder y decoder y la Dense layer del \n",
    "#decoder\n",
    "#Leer archivo\n",
    "\n",
    "\n",
    "#Introducimos una frase\n",
    "es_sentence=[\"de acuerdo con lo que se. \"]\n",
    "\n",
    "es_seq = sentences_to_sequences(es_tok,es_sentence,es_len, es_vocab)\n",
    "#Vamos a predecir el primer el estado a traves del encoder\n",
    "en_state = encoder.predict(es_seq)\n",
    "en_seq = word_to_onehot(en_tok, \"sos\", en_vocab)\n",
    "en_sentence = \"\"\n",
    "for i in range(en_len):\n",
    "    en_probability, en_state = decoder.predict([en_seq,en_state])\n",
    "    word = probability_to_word(en_tok,en_probability)\n",
    "    en_seq = word_to_onehot(en_tok, word, en_vocab)\n",
    "    if word == \"eos\": break\n",
    "    en_sentence += word+\" \"\n",
    "    print(word)\n",
    "print(en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
