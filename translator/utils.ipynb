{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En este archivo vamos a definir las diferentes funciones que vamos a usar en los diferentes modelos\n",
    "\n",
    "#---Imports----\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import layers as layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import random\n",
    "from unicodedata import normalize\n",
    "import string\n",
    "import os.path\n",
    "\n",
    "\n",
    "#---Definicion de parametros---\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "num_sentences = 1000\n",
    "latent_dim = 128\n",
    "hidden_units = 128\n",
    "model_path_lstm = \"lstm_model.h5\"\n",
    "model_path_gru_embedding = \"gru_embedding.h5\"\n",
    "model_path_gru_inference = \"gru_inference.h5\"\n",
    "es_dataset = \"../dataset/es-en/europarl-v7.es-en.es\"\n",
    "en_dataset = \"../dataset/es-en/europarl-v7.es-en.en\"\n",
    "es_tr_dataset = \"../dataset/es-en/es_tr_dataset.es\"\n",
    "es_test_dataset = \"../dataset/es-en/es_test_dataset.es\"\n",
    "en_tr_dataset = \"../dataset/es-en/en_tr_dataset.en\"\n",
    "en_test_dataset = \"../dataset/es-en/en_test_dataset.en\"\n",
    "en_vocab_max = 100\n",
    "es_vocab_max = 100\n",
    "oov = \"Unkn\"\n",
    "perc_train = 0.8\n",
    "\n",
    "#---Definicion de funciones---\n",
    "\n",
    "#-------Leer datasets---------\n",
    "def cleantexts(texts):\n",
    "    text_new = list()\n",
    "    for line in texts:\n",
    "        line = normalize(\"NFD\", line).encode(\"ascii\",\"ignore\")\n",
    "        line = line.decode(\"UTF-8\")\n",
    "        \n",
    "        #Vamos a añadir el token de principio y final de frase.\n",
    "        #line = \"\".join([\"sos \",line,\" eos\"])\n",
    "        line = line.lower()\n",
    "        line = line.translate(line.maketrans('', '', string.punctuation))\n",
    "        text_new.append(line)\n",
    "    return text_new\n",
    "\n",
    "def read_datasets():\n",
    "    \n",
    "    f_en = open(en_dataset, \"r\")\n",
    "    f_es = open(es_dataset, \"r\")\n",
    "    es_texts = f_es.readlines()\n",
    "    en_texts = f_en.readlines()\n",
    "    index = random.sample(range(0, len(es_texts)),num_sentences)\n",
    "    en_texts = [en_texts[ind] for ind in index]\n",
    "    es_texts = [es_texts[ind] for ind in index]\n",
    "    f_es.close()\n",
    "    f_en.close()\n",
    "    return(es_texts, en_texts)\n",
    "    f_es.close()\n",
    "    f_en.close()\n",
    "    return(es_texts, en_texts)\n",
    "\n",
    "def init_data():\n",
    "    if(os.path.isfile(es_tr_dataset) and os.path.isfile(en_tr_dataset) and os.path.isfile(es_test_dataset) and os.path.isfile(en_test_dataset)):\n",
    "        f_tr_es = open(es_tr_dataset,\"r\")\n",
    "        f_tr_en = open(en_tr_dataset, \"r\")\n",
    "        f_test_es = open(es_test_dataset,\"r\")\n",
    "        f_test_en = open(en_test_dataset,\"r\")\n",
    "        es_train = f_tr_es.readlines()\n",
    "        en_train = f_tr_en.readlines()\n",
    "        es_test = f_test_es.readlines()\n",
    "        en_test = f_test_en.readlines()\n",
    "        \n",
    "    else:\n",
    "        es_texts, en_texts = read_datasets()\n",
    "        es_texts = cleantexts(es_texts)\n",
    "        en_texts = cleantexts(en_texts)\n",
    "        es_train,es_test,en_train,en_test = split_dataset(es_texts,en_texts)\n",
    "        f_tr_es = open(es_tr_dataset,\"w\")\n",
    "        f_tr_en = open(en_tr_dataset, \"w\")\n",
    "        f_test_es = open(es_test_dataset,\"w\")\n",
    "        f_test_en = open(en_test_dataset,\"w\")\n",
    "        f_tr_es.writelines(es_train)\n",
    "        f_tr_en.writelines(en_train)\n",
    "        f_test_es.writelines(es_test)\n",
    "        f_test_en.writelines(en_test)\n",
    "    es_texts = es_train\n",
    "    es_texts.extend(es_test)\n",
    "    en_texts = en_train\n",
    "    en_texts.extend(en_test)\n",
    "        \n",
    "    return(es_train, en_train, es_test,en_test,es_texts,en_texts)\n",
    "\n",
    "def split_dataset(en_texts, es_texts):\n",
    "    train_num = int(num_sentences * perc_train)\n",
    "    test_num = int(num_sentences - train_num)\n",
    "    index = np.arange(len(es_texts))\n",
    "    np.random.shuffle(index)\n",
    "    train_index , test_index = index[:train_num], index[train_num:train_num+test_num]\n",
    "    \n",
    "    en_train = [en_texts[ind] for ind in train_index]\n",
    "    en_test = [en_texts[ind] for ind in test_index]\n",
    "    es_train =[es_texts[ind] for ind in train_index]\n",
    "    es_test =[es_texts[ind] for ind in test_index]\n",
    "    \n",
    "    return(es_train, es_test, en_train,en_test)\n",
    "\n",
    "\n",
    "#-------Tokenizar---------\n",
    "\n",
    "def init_tokens(en_texts,es_texts):\n",
    "    en_tok = Tokenizer(num_words=en_vocab_max, oov_token = oov)\n",
    "    es_tok = Tokenizer(num_words=es_vocab_max, oov_token = oov)\n",
    "    en_tok.fit_on_texts(en_texts)\n",
    "    es_tok.fit_on_texts(es_texts)\n",
    "    en_len = max(len(line.split()) for line in en_texts)\n",
    "    es_len = max(len(line.split()) for line in es_texts)\n",
    "    en_vocab = len(en_tok.word_index)+1\n",
    "    es_vocab = len(es_tok.word_index)+1\n",
    "    return(es_tok,en_tok,es_len,en_len,es_vocab,en_vocab)\n",
    "\n",
    "\n",
    "#-------Crear modelos-------\n",
    "\n",
    "#MODELO GRU CON INFERENCIA\n",
    "def create_model_inference_train(es_len, en_len, es_vocab, en_vocab):\n",
    "    #Modelo con inferencia \n",
    "    #-----\n",
    "    #Encoder\n",
    "    #-----\n",
    "    #Definimos la capa \"input\" para nuestro encoder\n",
    "    es_input_layer = layers.Input(shape=(es_len, es_vocab))\n",
    "\n",
    "    #Definimos la capa \"Gru\" para el encoder, tendremos que definir el tamaño (numero de neuronas)de la capa oculta \n",
    "    #que hemos ido probando a base de prueba y error\n",
    "    es_gru_layer = layers.GRU(hidden_units, return_state = True)\n",
    "\n",
    "    #Obtenemos la salida del encoder y el estado:\n",
    "    es_output, es_state = es_gru_layer(es_input_layer)\n",
    "\n",
    "    #-----\n",
    "    #Decoder\n",
    "    #-----\n",
    "    #Definimos la capa de entrada \"input\" del decoder\n",
    "    en_input_layer = layers.Input(shape=(en_len-1, en_vocab))\n",
    "    en_gru_layer = layers.GRU(hidden_units, return_sequences = True)\n",
    "    #Obtenemos la salida del decoder\n",
    "    en_output = en_gru_layer(en_input_layer, initial_state = es_state)\n",
    "\n",
    "    #Definimos una capa \"TimeDistributed\" con otra capa \"Dense\"\n",
    "    en_dense_layer = layers.TimeDistributed(layers.Dense(en_vocab,activation=\"softmax\"))\n",
    "    en_prediction = en_dense_layer(en_output)\n",
    "\n",
    "    #-----\n",
    "    #Creamos el modelo\n",
    "    #-----\n",
    "    model = Model(inputs=[es_input_layer, en_input_layer], outputs = en_prediction)\n",
    "    return(model)\n",
    "\n",
    "#MODELO GRU CON EMBEDDING\n",
    "def create_model_gru_embedding(es_len, en_len, es_vocab, en_vocab):\n",
    "    #-----\n",
    "    #Encoder\n",
    "    #-----\n",
    "    #Vamos a definir la capa de entrada, sin definir el batch_size\n",
    "    es_input_layer = layers.Input(shape=(es_len,))\n",
    "    #Vamos a definir la capa de embedding\n",
    "    es_embedding_layer = layers.Embedding(es_vocab, latent_dim,input_length=es_len)(es_input_layer)\n",
    "    #Definimos la salida y el estado:\n",
    "    es_output, es_output_state = layers.GRU(hidden_units, return_state = True)(es_embedding_layer)\n",
    "\n",
    "    #-----\n",
    "    #Decoder\n",
    "    #-----\n",
    "    #Definimos la capa de entrada \"input\" del decoder que acepta un solo onehot vector\n",
    "    en_input_layer = layers.Input(shape=(en_len-1,))\n",
    "    #Definimos en el decoder una campa embedding tambien que aceptara la entrada del decoder\n",
    "    en_embedding_layer = layers.Embedding(en_vocab, latent_dim, input_length=en_len-1)(en_input_layer)\n",
    "    en_output, _= layers.GRU(hidden_units, return_state = True, return_sequences = True)(en_embedding_layer, initial_state = es_output_state)\n",
    "\n",
    "    #Creamos una ultima capa que contiene 2 capas, una \"TimeDistributed\" y una \"Dense\"\n",
    "    en_prediction = layers.TimeDistributed(layers.Dense(en_vocab, activation = \"softmax\"))(en_output)\n",
    "\n",
    "    #Definimos el modelo del decoder\n",
    "    model = Model([es_input_layer,en_input_layer], en_prediction)\n",
    "    return(model)\n",
    "\n",
    "#MODELO LSTM CON EMBEDDING\n",
    "def create_model_lstm(es_len, en_len,es_vocab,en_vocab):\n",
    "    '''\n",
    "    #-----\n",
    "    #Encoder\n",
    "    #-----\n",
    "    es_input_layer = layers.Input(shape=(es_len,))\n",
    "    es_embedding_layer = layers.Embedding(es_vocab, latent_dim,input_length=es_len)(es_input_layer)\n",
    "    es_output,state_h,state_c = layers.LSTM(latent_dim,return_state=True)(es_embedding_layer)\n",
    "    \n",
    "    #-----\n",
    "    #Decoder\n",
    "    #-----\n",
    "    en_input_layer = layers.Input(shape=(en_len-1,))\n",
    "    en_embedding_layer = layers.Embedding(en_vocab, latent_dim, input_length=en_len-1)(en_input_layer)\n",
    "    en_lstm_layer = layers.LSTM(latent_dim, return_sequences = True)\n",
    "    en_output = en_lstm_layer(en_embedding_layer,initial_state=[state_h,state_c])\n",
    "    en_dense_layer = layers.TimeDistributed(layers.Dense(en_vocab,activation=\"softmax\"))\n",
    "    en_prediction=  en_dense_layer(en_output)\n",
    "    model = Model([es_input_layer,en_input_layer], en_prediction)\n",
    "    return(model)\n",
    "    '''\n",
    "    #---Encoder---\n",
    "    es_input_layer = layers.Input(shape=(None,))\n",
    "    es_embedding_layer = layers.Embedding(es_vocab, latent_dim, input_length=es_len)(es_input_layer)\n",
    "    es_output, state_h, state_c = layers.LSTM(latent_dim, return_state=True)(es_embedding_layer)\n",
    "    \n",
    "    #----Decoder--\n",
    "    en_input_layer = layers.Input(shape=(None,))\n",
    "    en_embedding_layer = layers.Embedding(en_vocab,latent_dim)(en_input_layer)\n",
    "    en_output = layers.LSTM(latent_dim, return_sequences = True)(en_embedding_layer, initial_state=[state_h,state_c])\n",
    "    en_prediction = layers.TimeDistributed(layers.Dense(en_vocab,activation=\"softmax\"))(en_output)\n",
    "    return(Model([es_input_layer,en_input_layer], en_prediction))\n",
    "    \n",
    "\n",
    "#----Entrenar Modelo----\n",
    "def train_inference_model():\n",
    "    \n",
    "    tr_es_x = sentences_to_sequences(es_tok, es_train, es_len,es_vocab)\n",
    "    tr_en_xy = sentences_to_sequences(en_tok, en_train, en_len, en_vocab, reverse = False)\n",
    "    tr_en_x = tr_en_xy[:,:-1,:]\n",
    "    tr_en_y = tr_en_xy[:,1:,:]\n",
    "    test_es_x = sentences_to_sequences(es_tok, es_test, es_len, es_vocab)\n",
    "    test_en_xy = sentences_to_sequences(en_tok, en_test, en_len, en_vocab,reverse=False)\n",
    "    test_en_x = test_en_xy[:,:-1,:]\n",
    "    test_en_y = test_en_xy[:,1:,:]\n",
    "    checkpoint = ModelCheckpoint(model_path_gru_inference, monitor =\"val_loss\", save_best_only=True)\n",
    "    model.fit([tr_es_x, tr_en_x], tr_en_y,batch_size=batch_size,epochs=epochs, validation_data=([test_es_x,test_en_x],test_en_y), callbacks = [checkpoint])\n",
    "    \n",
    "    '''\n",
    "    best = 0\n",
    "    for i in range(epochs):\n",
    "        for j in range(0,len(es_train),batch_size):\n",
    "            #Definimos el primer input, la entrada en español:\n",
    "            tr_es_x = sentences_to_sequences(es_tok, es_train[j:j+batch_size], es_len,es_vocab)\n",
    "            #Definimos el input en ingles junto al output\n",
    "            tr_en_xy = sentences_to_sequences(en_tok, en_train[j:j+batch_size], en_len, en_vocab, reverse = False)\n",
    "            #Tenemos que serparar la entrada de la salida\n",
    "            tr_en_x = tr_en_xy[:,:-1,:]\n",
    "            tr_en_y = tr_en_xy[:,1:,:]\n",
    "            #Entrenamos el modelo para este conjunto.\n",
    "            model.train_on_batch([tr_es_x,tr_en_x],tr_en_y)\n",
    "        #Obtenemos los inputs y output para validar el modelo en este epoch\n",
    "        test_es_x = sentences_to_sequences(es_tok, es_test, es_len, es_vocab)\n",
    "        test_en_xy = sentences_to_sequences(en_tok, en_test, en_len, en_vocab,reverse=False)\n",
    "        #Separamos la salida y la entrada\n",
    "        test_en_x = test_en_xy[:,:-1,:]\n",
    "        test_en_y = test_en_xy[:,1:,:]\n",
    "        #Validamos el modelo\n",
    "        ev = model.evaluate([test_es_x,test_en_x], test_en_y, batch_size=test_num,verbose=0)\n",
    "        print(\"Loss, Acc\", ev)\n",
    "        if(ev[0]<best or best == 0):\n",
    "            model.save(\"inference_model.h5\")\n",
    "            best = ev[0]\n",
    "    '''\n",
    "\n",
    "def train_gru_embedding_model():\n",
    "\n",
    "    tr_es_x = sentences_to_sequences(es_tok, es_train, es_len,es_vocab, onehot=False)\n",
    "    #Definimos el input en ingles junto al output\n",
    "    tr_en_xy = sentences_to_sequences(en_tok, en_train, en_len, en_vocab,onehot=False, reverse = False)\n",
    "    #Tenemos que serparar la entrada de la salida\n",
    "    tr_en_x = tr_en_xy[:,:-1]\n",
    "    #Para la salida tenemos que transformarla en onehot vector\n",
    "    tr_en_xy_onehot = sentences_to_sequences(en_tok, en_train, en_len, en_vocab, reverse = False)\n",
    "    tr_en_y = tr_en_xy_onehot[:,1:,:]\n",
    "    test_es_x = sentences_to_sequences(es_tok, es_test, es_len, es_vocab,onehot=False)\n",
    "    test_en_xy = sentences_to_sequences(en_tok, en_test, en_len, en_vocab,onehot=False,reverse=False)\n",
    "    #Separamos la salida y la entrada\n",
    "    test_en_x = test_en_xy[:,:-1]\n",
    "    test_en_xy_onehot = test_en_xy = sentences_to_sequences(en_tok, en_test, en_len, en_vocab,reverse=False)\n",
    "    test_en_y = test_en_xy_onehot[:,1:,:]\n",
    "    checkpoint = ModelCheckpoint(model_path_gru_embedding, monitor =\"val_loss\", save_best_only=True) \n",
    "    model.fit([tr_es_x, tr_en_x], tr_en_y,batch_size=batch_size,epochs=epochs, validation_data=([test_es_x,test_en_x],test_en_y), callbacks=[checkpoint])\n",
    "    '''\n",
    "    best = 0\n",
    "    for i in range(epochs):\n",
    "        for j in range(0,len(es_train),batch_size):\n",
    "            #Definimos el primer input, la entrada en español:\n",
    "            tr_es_x = sentences_to_sequences(es_tok, es_train[j:j+batch_size], es_len,es_vocab, onehot=False)\n",
    "            #Definimos el input en ingles junto al output\n",
    "            tr_en_xy = sentences_to_sequences(en_tok, en_train[j:j+batch_size], en_len, en_vocab,onehot=False, reverse = False)\n",
    "            #Tenemos que serparar la entrada de la salida\n",
    "            tr_en_x = tr_en_xy[:,:-1]\n",
    "            #Para la salida tenemos que transformarla en onehot vector\n",
    "            tr_en_xy_onehot = sentences_to_sequences(en_tok, en_train[j:j+batch_size], en_len, en_vocab, reverse = False)\n",
    "            tr_en_y = tr_en_xy_onehot[:,1:,:]\n",
    "            model.train_on_batch([tr_es_x,tr_en_x],tr_en_y)\n",
    "        #Obtenemos los inputs y output para validar el modelo en este epoch\n",
    "        test_es_x = sentences_to_sequences(es_tok, es_test, es_len, es_vocab,onehot=False)\n",
    "        test_en_xy = sentences_to_sequences(en_tok, en_test, en_len, en_vocab,onehot=False,reverse=False)\n",
    "        #Separamos la salida y la entrada\n",
    "        test_en_x = test_en_xy[:,:-1]\n",
    "        test_en_xy_onehot = test_en_xy = sentences_to_sequences(en_tok, en_test, en_len, en_vocab,reverse=False)\n",
    "        test_en_y = test_en_xy_onehot[:,1:,:]\n",
    "        #Validamos el modelo\n",
    "        ev = model.evaluate([test_es_x,test_en_x], test_en_y, batch_size=test_num,verbose=0)\n",
    "        print(\"Loss, Acc\", ev)\n",
    "        if(ev[0]<best or best == 0):\n",
    "            model.save(\"gru_embedding_model.h5\")\n",
    "            best = ev[0]\n",
    "    '''\n",
    "            \n",
    "def train_lstm_model():\n",
    "    tr_es_x = sentences_to_sequences(es_tok, es_train, es_len,es_vocab,onehot=False)\n",
    "    tr_en_x = sentences_to_sequences(en_tok, en_train, en_len, en_vocab, onehot=False, reverse = False)\n",
    "    tr_en_x = tr_en_x[:,:-1]\n",
    "    tr_en_y = sentences_to_sequences(en_tok, en_train, en_len, en_vocab, reverse = False)\n",
    "    tr_en_y = tr_en_y[:,1:,:]\n",
    "    \n",
    "    test_es_x = sentences_to_sequences(es_tok, es_test, es_len,es_vocab,onehot=False)\n",
    "    test_en_x = sentences_to_sequences(en_tok, en_test, en_len, en_vocab, onehot=False, reverse = False)\n",
    "    test_en_x = test_en_x[:,:-1]\n",
    "    test_en_y = sentences_to_sequences(en_tok, en_test, en_len, en_vocab, reverse = False)\n",
    "    test_en_y = test_en_y[:,1:,:]\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(model_path_lstm, monitor =\"val_loss\", save_best_only=True)\n",
    "    model.fit([tr_es_x, tr_en_x], tr_en_y,batch_size=batch_size,epochs=epochs, validation_data=([test_es_x,test_en_x],test_en_y), callbacks=[checkpoint])\n",
    "    \n",
    "    \n",
    "#---------Utils---------\n",
    "\n",
    "#Definimos una funcion que transformara una frase en una secuencia(vector de ids de la frase)\n",
    "#Se le aplicara un padding a la secuencia y si se quiere que sea un vector onehot se le aplicara la funcion\n",
    "#to_categorical dada por keras\n",
    "#Tendra la opcion de modificar el tipo de padding a pre o post\n",
    "#Modificar si quiere que sea onehot o no\n",
    "#Cambiar si se inivierte la frase\n",
    "def sentences_to_sequences(tokenizer,sentence,length,vocab_size, onehot=True, pad_type = \"post\", reverse = True):\n",
    "    seqs = tokenizer.texts_to_sequences(sentence)\n",
    "    seqs = pad_sequences(seqs, padding = pad_type, truncating=\"post\", maxlen = length)\n",
    "    if reverse:\n",
    "        seqs = seqs[:,::-1]\n",
    "    if onehot:\n",
    "        seqs = to_categorical(seqs,num_classes = vocab_size)\n",
    "    return seqs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
