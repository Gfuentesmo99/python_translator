{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Español- Numero de palabras:  4050 \t Frase mas larga:  102\n",
      "Ingles - Numero de palabras:  4841 \t Frase mas larga:  105\n"
     ]
    }
   ],
   "source": [
    "%run utils.ipynb\n",
    "#Leemos el dataset reducido\n",
    "es_train, es_train, es_test, en_test,es_texts,en_texts = init_data()\n",
    "\n",
    "\n",
    "#Ahora vamos a tokenizar los diferentes datasets para ello usaremos Keras y su Tokenizer\n",
    "es_tok,en_tok,es_len,en_len,es_vocab,en_vocab = init_tokens(en_texts,es_texts)\n",
    "\n",
    "print(\"Español- Numero de palabras: \", es_vocab, \"\\t Frase mas larga: \", es_len)\n",
    "print(\"Ingles - Numero de palabras: \", en_vocab, \"\\t Frase mas larga: \", en_len)\n",
    "\n",
    "#Dividimos el dataset en entrenamiento y prueba\n",
    "es_train, es_test, en_train, en_test = split_dataset(es_texts,en_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 102, 4050)]       0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  [(None, 128), (None, 128) 1605120   \n",
      "=================================================================\n",
      "Total params: 1,605,120\n",
      "Trainable params: 1,605,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 1, 4841)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_6 (GRU)                     [(None, 128), (None, 1908864     input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4841)         624489      gru_6[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,533,353\n",
      "Trainable params: 2,533,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%run utils.ipynb\n",
    "from tensorflow.keras import layers as layers\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#Ahora vamos a modificar el modelo para que sea capaz de traducir una frase que nosotros le introduzcamos.\n",
    "#Vamos a añadir recursividad al decoder\n",
    "trained_model = load_model(\"gru_inference.h5\")\n",
    "\n",
    "#-----\n",
    "#Encoder\n",
    "#-----\n",
    "#Es exactamente igual que el del anterior:\n",
    "#Definimos la capa \"input\" para nuestro encoder\n",
    "es_input_layer = layers.Input(shape=(es_len, es_vocab))\n",
    "\n",
    "#Definimos la capa \"Gru\" para el encoder, tendremos que definir el tamaño (numero de neuronas)de la capa oculta \n",
    "#que hemos ido probando a base de prueba y error\n",
    "hidden_units = 128\n",
    "es_gru_layer = layers.GRU(hidden_units, return_state = True, weights = trained_model.get_layer(index=2).get_weights())\n",
    "\n",
    "#Obtenemos la salida del encoder y el estado:\n",
    "es_output, es_state = es_gru_layer(es_input_layer)\n",
    "\n",
    "#Pero aqui creamos un modelo solo para el encoder\n",
    "encoder = Model(inputs=es_input_layer, outputs = es_state)\n",
    "encoder.summary()\n",
    "#-----\n",
    "#Decoder\n",
    "#-----\n",
    "#Es en el decoder donde vendrán las modificaciones\n",
    "#Creamos 2 capas de entradas:\n",
    "#La primera aceptara un solo vector onehot\n",
    "en_input_layer = layers.Input(shape=(1,en_vocab))\n",
    "#La otra aceptara una entrada para el estado t-1\n",
    "en_state_in = layers.Input(shape=(hidden_units,))\n",
    "#Definimos la capa Gru\n",
    "en_gru_layer = layers.GRU(hidden_units, return_state = True, weights = trained_model.get_layer(index=3).get_weights())\n",
    "#Obtenemos la salida y estado de la capa GRU\n",
    "en_output, en_output_state = en_gru_layer(en_input_layer, initial_state = en_state_in)\n",
    "#Definimos una ultima capa Dense\n",
    "en_dense_layer = layers.Dense(en_vocab,activation=\"softmax\",weights = trained_model.get_layer(index=4).get_weights())\n",
    "en_prediction = en_dense_layer(en_output)\n",
    "\n",
    "decoder = Model(inputs=[en_input_layer, en_state_in], outputs =[en_prediction, en_output_state])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-83a4989f3004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0men_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobability_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_tok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0men_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eos\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-83a4989f3004>\u001b[0m in \u001b[0;36mprobability_to_word\u001b[0;34m(tokenizer, prob)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprobability_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Ahora lo siguiente que tenemos que hacer es cargar el modelo entrenado (\"Modelo anterior a este\") y copiar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import numpy as np\n",
    "\n",
    "def word_to_onehot(tokenizer, word, vocab_size):\n",
    "    seq = tokenizer.texts_to_sequences([[word]])\n",
    "    seq = to_categorical(seq, num_classes=vocab_size)\n",
    "    seq = np.expand_dims(seq, axis=1)\n",
    "    return seq\n",
    "\n",
    "def probability_to_word(tokenizer, prob):\n",
    "    word_index = np.argmax(prob[0,:], axis=-1)\n",
    "    word = tokenizer.index_word[word_index]\n",
    "    return word\n",
    "#Ahora lo siguiente que tenemos que hacer es cargar el modelo entrenado (\"Modelo anterior a este\") y copiar\n",
    "#los pesos\n",
    "#En nuestro nuevo modelo hay 3 capas que contienen pesos, la Gru layer del encoder y decoder y la Dense layer del \n",
    "#decoder\n",
    "#Leer archivo\n",
    "\n",
    "\n",
    "#Introducimos una frase\n",
    "es_sentence=[\"de acuerdo con lo que se. \"]\n",
    "\n",
    "es_seq = sentences_to_sequences(es_tok,es_sentence,es_len, es_vocab)\n",
    "#Vamos a predecir el primer el estado a traves del encoder\n",
    "en_state = encoder.predict(es_seq)\n",
    "en_seq = word_to_onehot(en_tok, \"sos\", en_vocab)\n",
    "en_sentence = \"\"\n",
    "for i in range(en_len):\n",
    "    en_probability, en_state = decoder.predict([en_seq,en_state])\n",
    "    word = probability_to_word(en_tok,en_probability)\n",
    "    en_seq = word_to_onehot(en_tok, word, en_vocab)\n",
    "    if word == \"eos\": break\n",
    "    en_sentence += word+\" \"\n",
    "    print(word)\n",
    "print(en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
